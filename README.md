
# ã€é¡¹ç›®å®æˆ˜ã€‘å¤§æ¨¡å‹å¾®è°ƒ-æƒ…ç»ªå¯¹è¯æ¨¡å‹

[![Author](https://img.shields.io/badge/Author-å¢¨å®‡Logic-blue.svg)](https://blog.ismoyu.cn)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE) <!-- è¯·ç¡®ä¿é¡¹ç›®ä¸­åŒ…å«LICENSEæ–‡ä»¶ -->
![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-orange?logo=pytorch&logoColor=white)
![XTuner](https://img.shields.io/badge/Framework-XTuner-brightgreen)
![LMDeploy](https://img.shields.io/badge/Deployment-LMDeploy-purple)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-ff69b4)

## ğŸ“– é¡¹ç›®ç®€ä»‹ä¸ç›®æ ‡

æœ¬é¡¹ç›®æ—¨åœ¨å¸¦é¢†å­¦ä¹ è€…å®Œæˆä¸€ä¸ª**å…·å¤‡å¼ºçƒˆå’Œä¸ªæ€§åŒ–æƒ…ç»ªè¡¨è¾¾èƒ½åŠ›**çš„å¯¹è¯å¤§æ¨¡å‹çš„å¾®è°ƒå…¨æµç¨‹ã€‚çµæ„Ÿæ¥æºäºæŠ–éŸ³ä¸Šçš„â€œå°æ™ºèŠå¤©æœºå™¨äººâ€ï¼Œç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½åœ¨å¯¹è¯ä¸­å±•ç°ç‰¹å®šæƒ…ç»ªï¼ˆå¦‚â€œæ¸©æŸ”â€ã€â€œæ¯’èˆŒâ€ï¼‰çš„æ¨¡å‹ã€‚é¡¹ç›®è¦†ç›–ä»æ•°æ®å‡†å¤‡ã€æ¨¡å‹é€‰å‹ã€ä½¿ç”¨XTunerè¿›è¡Œè®­ç»ƒã€è¯„ä¼°åˆ°æœ€ç»ˆä½¿ç”¨LMDeployå’ŒStreamlitè¿›è¡Œéƒ¨ç½²ä¸äº¤äº’æµ‹è¯•çš„å®Œæ•´é“¾è·¯ã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**

*   æ„å»ºé«˜è´¨é‡ã€ç‰¹å®šæƒ…ç»ªé£æ ¼çš„å¯¹è¯æ•°æ®é›†ã€‚
*   é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæœ¬é¡¹ç›®é€‰ç”¨Qwen1.5-1.8B-Chatï¼‰å¹¶å¾®è°ƒä»¥å¢å¼ºå…¶æƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›ã€‚
*   æŒæ¡å¤§æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒæŠ€æœ¯å’Œå·¥å…·é“¾ï¼ˆXTuner, OpenCompass, LMDeployï¼‰ã€‚
*   å®ç°æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²å’Œäº¤äº’å¼æµ‹è¯•ã€‚
*   æ¢ç´¢æ¨¡å‹åœ¨å¿ƒç†è¾…æœã€ç”µå•†å®¢æœç­‰åœºæ™¯çš„åº”ç”¨æ½œåŠ›ã€‚

## âœ¨ é¡¹ç›®ç‰¹æ€§

*   **ç«¯åˆ°ç«¯å®è·µï¼š** å®Œæ•´è¦†ç›–LLMå¾®è°ƒçš„å„ä¸ªå…³é”®ç¯èŠ‚ï¼Œæä¾›å®è´µçš„åŠ¨æ‰‹ç»éªŒã€‚
*   **é—®é¢˜é©±åŠ¨ï¼š** ä»¥è§£å†³å®é™…åº”ç”¨ä¸­æƒ…ç»ªåŒ–è¡¨è¾¾çš„éœ€æ±‚ä¸ºå‡ºå‘ç‚¹ã€‚
*   **å®ç”¨å·¥å…·é“¾ï¼š** é€‰ç”¨ä¸šç•Œä¸»æµçš„XTunerã€OpenCompassã€LMDeployç­‰å·¥å…·ã€‚
*   **é‡è§†æ•°æ®è´¨é‡ï¼š** è¯¦ç»†ä»‹ç»AIè¾…åŠ©ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†åŠå¤šç»´åº¦éªŒè¯æ–¹æ³•ã€‚
*   **è¿­ä»£ä¼˜åŒ–ï¼š** è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜ã€è§£å†³æ–¹æ¡ˆåŠå¤šæ¬¡è®­ç»ƒçš„ç»éªŒæ€»ç»“ã€‚
*   **æƒ…ç»ªå¯å®šåˆ¶ï¼š** é€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰å’Œç”Ÿæˆä¸åŒé£æ ¼ï¼ˆå¦‚æ¸©æŸ”ã€æ¯’èˆŒï¼‰çš„å¯¹è¯æ•°æ®ã€‚

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

| ç»„ä»¶ (Component)     | æŠ€æœ¯/å·¥å…· (Technology/Tool)                   | è§’è‰²/ç”¨é€” (Role/Purpose)                                     |
| :------------------- | :-------------------------------------------- | :----------------------------------------------------------- |
| ç¼–ç¨‹è¯­è¨€ (Language)  | Python 3.10+                                  | ä¸»è¦å¼€å‘è¯­è¨€                                                 |
| æ•°æ®ç”Ÿæˆ (Data Gen)  | ZhipuAI API (glm-3-turbo)                     | AIè¾…åŠ©ç”Ÿæˆæƒ…ç»ªå¯¹è¯æ•°æ®                                       |
| æ–‡æœ¬åµŒå…¥ (Embeddings) | `thomas/text2vec-base-chinese`                | ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦æ ¡éªŒ                                           |
| æ¨¡å‹è¯„ä¼° (Evaluation) | OpenCompass                                   | è¯„ä¼°å€™é€‰æ¨¡å‹çš„ä¸­æ–‡ç†è§£èƒ½åŠ› (CLUEæ•°æ®é›†)                      |
| æ¨¡å‹å¾®è°ƒ (Fine-tuning) | XTuner                                        | ä¸»è¦çš„å¾®è°ƒæ¡†æ¶ï¼Œæ”¯æŒQLoRA                                    |
| æ¨¡å‹éƒ¨ç½² (Deployment) | LMDeploy                                      | å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œç”¨äºæœåŠ¡åŒ–éƒ¨ç½²                               |
| å‰ç«¯äº¤äº’ (Frontend)   | Streamlit                                     | å¿«é€Ÿæ­å»ºäº¤äº’å¼Webåº”ç”¨ï¼Œç”¨äºæ¨¡å‹æ•ˆæœæµ‹è¯•                      |
| é…ç½®ç®¡ç† (Config)    | YAML, JSON                                    | å­˜å‚¨APIå¯†é’¥ã€è·¯å¾„ã€é£æ ¼æ¨¡æ¿ç­‰é…ç½®                            |
| æ“ä½œç³»ç»Ÿ (OS)        | Windows11 + WSL2 (Ubuntu 22.04)               | å¼€å‘ä¸è®­ç»ƒç¯å¢ƒ (æ¨èLinuxç¯å¢ƒ)                               |
| ç¡¬ä»¶ (Hardware)      | GeForce RTX 4060 Ti 16GB (æˆ–åŒç­‰çº§æ˜¾å¡)       | æ¨¡å‹è®­ç»ƒç¡¬ä»¶                                                 |

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
emotion_dialogue_tuner/
â”œâ”€â”€ config/                         # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ settings.yaml               # APIå¯†é’¥ç­‰æ•æ„Ÿé…ç½®
â”‚   â””â”€â”€ style_config.json           # é£æ ¼æ¨¡æ¿é…ç½®
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ LCCC-base-split/            # å…¬å¼€åŸå§‹æ•°æ®é›†
â”‚   â”œâ”€â”€ cleaned_output.txt          # å¤„ç†åç”¨æˆ·å¯¹è¯æ•°æ®
â”œâ”€â”€ embedding_model/                # embeddingæ¨¡å‹è·¯å¾„
â”œâ”€â”€ outputs/                        # ç”Ÿæˆæ•°æ®ä¿å­˜ç›®å½•
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generator.py           # æ•°æ®ç”Ÿæˆæ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ chat_template_utils.py  # æ¨¡å‹å¯¹è¯æ¨¡æ¿è½¬æ¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ config_loader.py        # é…ç½®åŠ è½½æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ data_convert.py         # è®­ç»ƒæ•°æ®æ ¼å¼è½¬æ¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ data_extraction.py      # æå–æ¸…æ´—æ•°æ®æ¨¡å—
â”‚   â”‚   â””â”€â”€ validator.py            # å›å¤è´¨é‡æ ¡éªŒ
â””â”€â”€ main.py                         # ä¸»æ‰§è¡Œå…¥å£
```

## ğŸš€ ç¯å¢ƒå‡†å¤‡ä¸å®‰è£…æŒ‡å—

1.  **åŸºç¡€ç¯å¢ƒï¼š**
    *   æ¨èä½¿ç”¨ Linux (å¦‚ Ubuntu 22.04) æˆ– Windows 11 + WSL2 (Ubuntu 22.04)ã€‚
    *   NVIDIA æ˜¾å¡ï¼Œå¹¶å®‰è£…å¯¹åº”çš„ [NVIDIAé©±åŠ¨](https://www.nvidia.com/Download/index.aspx) å’Œ [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive) (æ•™ç¨‹ä¸­æåŠCUDA 12.1ï¼Œè¯·æ ¹æ®XTunerå’ŒPyTorchç‰ˆæœ¬å…¼å®¹æ€§é€‰æ‹©)ã€‚

2.  **Condaç¯å¢ƒ (æ¨è)ï¼š**
    ```bash
    conda create -n emotion_llm_env python=3.10 -y
    conda activate emotion_llm_env
    ```

3.  **å®‰è£…æ ¸å¿ƒä¾èµ–ï¼š**
    *   **PyTorch:** æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬ä» [PyTorchå®˜ç½‘](https://pytorch.org/get-started/locally/) è·å–å®‰è£…å‘½ä»¤ã€‚æ•™ç¨‹ä¸­å› ç‰ˆæœ¬å…¼å®¹é—®é¢˜ï¼Œæœ€ç»ˆä½¿ç”¨äº†ï¼š
        ```bash
        pip install torch==2.2.0+cu121 torchvision==0.17.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121
        ```
    *   **XTuner:**
        ```bash
        # å…‹éš†XTunerä»“åº“ (å¦‚æœéœ€è¦ä»æºç å®‰è£…æˆ–ä¿®æ”¹)
        # git clone https://github.com/InternLM/xtuner.git
        # cd xtuner
        # pip install -e '.[all]'

        # æˆ–è€…ç›´æ¥pipå®‰è£… (å¯èƒ½ç‰ˆæœ¬ä¸æ•™ç¨‹æœ‰å·®å¼‚)
        pip install xtuner[all] -U
        ```
        *æ³¨æ„ï¼šæ•™ç¨‹ä¸­é‡åˆ°`bitsandbytes`å’Œ`triton.ops`çš„ä¾èµ–é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´PyTorchå’ŒCUDAç‰ˆæœ¬ï¼Œå¹¶é‡æ–°å®‰è£…XTunerä¾èµ–è§£å†³ã€‚è¯·åŠ¡å¿…å…³æ³¨XTunerå®˜æ–¹æ–‡æ¡£å…³äºç¯å¢ƒé…ç½®çš„è¯´æ˜ã€‚*

    *   **LMDeploy:**
        ```bash
        pip install lmdeploy
        ```
    *   **OpenCompass (å¯é€‰ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°):**
        ```bash
        # å…‹éš†OpenCompassä»“åº“
        # git clone https://github.com/open-compass/opencompass.git
        # cd opencompass
        # pip install -r requirements.txt
        # python setup.py develop
        ```
    *   **Streamlit å’Œå…¶ä»–:**
        ```bash
        pip install streamlit openai zhipuai pyyaml sentence_transformers numpy
        ```
    *   **é¡¹ç›®ä¾èµ–:**
        å…‹éš†æœ¬é¡¹ç›®åï¼Œå¯ä»¥åˆ›å»º`requirements.txt`æ–‡ä»¶å¹¶ä½¿ç”¨`pip install -r requirements.txt`å®‰è£…ã€‚

4.  **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼š**
    *   **Qwen1.5-1.8B-Chat:** ä»Hugging Face Hubæˆ–ModelScopeä¸‹è½½ï¼Œå¹¶æ”¾ç½®åˆ°æŒ‡å®šè·¯å¾„ (å¦‚ `llm/Qwen/Qwen1___5-1___8B-Chat`)ã€‚
    *   **text2vec-base-chinese:** ä»ModelScope (`thomas/text2vec-base-chinese`)ä¸‹è½½ï¼Œå¹¶æ”¾ç½®åˆ°æŒ‡å®šè·¯å¾„ (å¦‚ `embedding_model/thomas/text2vec-base-chinese`)ã€‚

## ğŸ“Š æ•°æ®å‡†å¤‡æ­¥éª¤

1.  **æ”¶é›†åŸå§‹é—®é¢˜ï¼š**
    *   ä» [CDial-GPT](https://github.com/thu-coai/CDial-GPT) æˆ– é­”å¡”ç¤¾åŒº [LCCC](https://www.modelscope.cn/datasets/OmniData/LCCC) ç­‰æ•°æ®é›†ä¸­æå–å¯¹è¯çš„é—®é¢˜éƒ¨åˆ†ã€‚
    *   å°†è¿™äº›é—®é¢˜æ•´ç†åˆ°ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œä¾‹å¦‚ `data/cleaned_output.txt`ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ã€‚

2.  **é…ç½®APIå¯†é’¥å’Œè·¯å¾„ (`config/settings.yaml`)ï¼š**
    ```yaml
    API:
      ZHIPU_API_KEY: "your_zhipu_api_key_here"  # æ›¿æ¢ä¸ºä½ çš„æ™ºè°±AI API Key
      MODEL_NAME: "glm-3-turbo"

    PATHS:
      EMBEDDING_MODEL: "path/to/your/embedding_model/thomas/text2vec-base-chinese" # æ›¿æ¢ä¸ºä½ çš„åµŒå…¥æ¨¡å‹æœ¬åœ°è·¯å¾„
    ```

3.  **é…ç½®å¯¹è¯é£æ ¼ (`config/style_config.json`)ï¼š**
    å®šä¹‰AIç”Ÿæˆæ•°æ®æ—¶åº”éµå¾ªçš„ç³»ç»Ÿæç¤ºå’Œå¯¹è¯ç¤ºä¾‹ã€‚
    ```json
    {
        "æ¸©æŸ”": {
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´çš„èŠå¤©åŠ©æ‰‹...",
            "examples": [
                {"role": "user", "content": "ä»Šå¤©å¥½ç´¯å•Š"},
                {"role": "assistant", "content": "è¾›è‹¦å•¦~ è¦ç»™è‡ªå·±æ³¡æ¯çƒ­èŒ¶æ”¾æ¾ä¸€ä¸‹å—ï¼ŸğŸŒ¸"}
            ],
            "temperature": 0.3
        },
        "æ¯’èˆŒ": {
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢ç”¨çŠ€åˆ©åæ§½è¡¨è¾¾å…³å¿ƒçš„æœ‹å‹...",
            "examples": [
                {"role": "user", "content": "åˆèƒ–äº†5æ–¤ï¼"},
                {"role": "assistant", "content": "å¥½å®¶ä¼™ï¼ä½ è¿™æ˜¯è¦æŠŠä½“é‡ç§¤å‹æˆåˆ†å­æ–™ç†ï¼ŸğŸ‹ï¸"}
            ],
            "temperature": 0.7
        }
    }
    ```

4.  **è¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬ï¼š**
    ç¡®ä¿ `emotion_dialogue_tuner/main.py` ä¸­çš„æ–‡ä»¶è·¯å¾„æ­£ç¡®æŒ‡å‘æ‚¨çš„è¾“å…¥é—®é¢˜æ–‡ä»¶ã€‚
    ```bash
    cd emotion-dialogue-finetune/emotion_dialogue_tuner/
    python main.py
    ```
    ç”Ÿæˆçš„å¸¦é£æ ¼æ ‡ç­¾çš„å¯¹è¯æ•°æ®å°†ä¿å­˜åˆ° `outputs/style_chat_data.json` (æˆ–åœ¨`main.py`ä¸­æŒ‡å®šçš„å…¶ä»–è·¯å¾„)ã€‚XTunerè®­ç»ƒæ—¶å¯èƒ½éœ€è¦å°†å…¶é‡å‘½åæˆ–é…ç½®æ–‡ä»¶æŒ‡å‘æ­¤æ–‡ä»¶ï¼Œä¾‹å¦‚ `output.json`ã€‚

## ğŸ¤– æ¨¡å‹è®­ç»ƒæ­¥éª¤ (XTuner)

1.  **å‡†å¤‡XTuneré…ç½®æ–‡ä»¶ï¼š**
    *   å¤åˆ¶ä¸€ä»½XTunerå®˜æ–¹æä¾›çš„Qwen1.5çš„QLoRAé…ç½®æ–‡ä»¶ï¼Œä¾‹å¦‚ä» `xtuner/configs/qwen/qwen1_5_1_8b_chat/qwen1_5_1_8b_chat_qlora_alpaca_e3.py` å¤åˆ¶åˆ° `xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py`ã€‚
    *   **ä¿®æ”¹é…ç½®æ–‡ä»¶ (`xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py`):**
        ```python
        # PART 1: Settings
        # ...
        pretrained_model_name_or_path = "/path/to/your/Qwen1.5-1.8B-Chat"  # æ›¿æ¢ä¸ºä½ çš„åŸºåº§æ¨¡å‹è·¯å¾„
        # ...
        alpaca_en_path = "/path/to/your/generated_data/output.json"  # æ›¿æ¢ä¸ºä½ çš„å¾®è°ƒæ•°æ®è·¯å¾„
        # ...
        max_length = 512
        batch_size = 4  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
        max_epochs = 3000 # æ•™ç¨‹ä¸­å‘ç°è¿‡æ‹Ÿåˆï¼Œå®é™…å¯èƒ½åœ¨6000-7000 iter (éepoch) æ•ˆæœè¾ƒå¥½
        # ...
        evaluation_inputs = [ # ç”¨äºè®­ç»ƒä¸­ä¸»è§‚è¯„ä¼°çš„é—®é¢˜
            "ç”·æœ‹å‹ç»™å¥³ä¸»æ’­åˆ·ç«ç®­ï¼Œç®—ç²¾ç¥å‡ºè½¨å—ï¼Ÿ",
            "å–çº¢é…’å…»ç”Ÿï¼Œç»“æœå–åˆ°å¤´æ™•â€¦",
            # ...æ›´å¤šè¯„ä¼°é—®é¢˜
        ]
        # ...

        # PART 2: Model & Tokenizer
        # ...
        # æ ¹æ®æ•™ç¨‹ç»éªŒï¼Œä¿®æ”¹QLoRAå‚æ•°ï¼Œä¾‹å¦‚å¯ç”¨8-bitè€Œé4-bit
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_8bit=True,  # æ•™ç¨‹ä¸­ä»Falseæ”¹ä¸ºTrue
            # load_in_4bit=False, # æ•™ç¨‹ä¸­ä»Trueæ”¹ä¸ºFalse
            # llm_int8_threshold=6.0,
            # llm_int8_has_fp16_weight=False,
            # bnb_4bit_compute_dtype=torch.float16,
            # bnb_4bit_use_double_quant=True,
            # bnb_4bit_quant_type='nf4'
            ),
        # ...
        lora_config=dict(
            type=LoraConfig,
            r=32, # æ•™ç¨‹ä¸­ä½¿ç”¨32
            lora_alpha=64, # æ•™ç¨‹ä¸­ä½¿ç”¨64
            lora_dropout=0.05,
            bias='none',
            task_type='CAUSAL_LM'
        )
        # ...

        # PART 3: Dataset & Dataloader
        # ...
        # ç¡®ä¿æ•°æ®é›†ç±»å‹å’Œè·¯å¾„æ­£ç¡®
        dataset=dict(type=load_dataset, path="json", data_files=dict(train=alpaca_en_path)),
        # ...
        ```
        *è¯·ä»”ç»†å‚è€ƒæ•™ç¨‹ä¸­å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯QLoRAå‚æ•°å’Œæ•°æ®é›†åŠ è½½éƒ¨åˆ†ã€‚*

2.  **å¯åŠ¨è®­ç»ƒï¼š**
    ```bash
    # è¿›å…¥XTuneré…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½•æˆ–æä¾›å®Œæ•´è·¯å¾„
    # å•å¡è®­ç»ƒ
    xtuner train xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py

    # åå°è¿è¡Œå¹¶å°†æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶
    nohup xtuner train xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py > train_$(date +%Y%m%d_%H%M%S).log 2>&1 &
    ```
    è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨æ„è§‚å¯Ÿlosså˜åŒ–å’Œ`evaluation_inputs`çš„è¾“å‡ºï¼Œæ•™ç¨‹ç»éªŒè¡¨æ˜åœ¨6000-7000è½®æ¬¡ï¼ˆiterationsï¼‰æ—¶lossçº¦0.03-0.04ï¼Œæ•ˆæœè¾ƒå¥½ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

##ğŸ”„ æ¨¡å‹è½¬æ¢ä¸åˆå¹¶

è®­ç»ƒå®Œæˆåï¼ŒLoRAæƒé‡ä¿å­˜åœ¨ `work_dirs` ä¸‹å¯¹åº”é…ç½®åç§°çš„ç›®å½•ä¸­ (å¦‚ `work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/iter_XXXX.pth`)ã€‚

1.  **PTHè½¬HuggingFace Adapteræ ¼å¼ï¼š**
    ```bash
    # FINETUNE_CFG: ä½ çš„XTuneré…ç½®æ–‡ä»¶è·¯å¾„
    # PTH_PATH: è®­ç»ƒå¾—åˆ°çš„.pthæƒé‡æ–‡ä»¶è·¯å¾„
    # SAVE_PATH: è½¬æ¢åAdapterçš„ä¿å­˜è·¯å¾„

    xtuner convert pth_to_hf \
        xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py \
        work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/iter_7000.pth \
        work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/iter_7000_hf_adapter
    ```

2.  **Adapterä¸åŸºåº§æ¨¡å‹åˆå¹¶ (ç”Ÿæˆå®Œæ•´æ¨¡å‹æƒé‡)ï¼š**
    ```bash
    # LLM_BASE_PATH: åŸå§‹Qwen1.5-1.8B-ChatåŸºåº§æ¨¡å‹è·¯å¾„
    # ADAPTER_PATH: ä¸Šä¸€æ­¥è½¬æ¢å¾—åˆ°çš„Adapterè·¯å¾„
    # SAVE_MERGED_PATH: åˆå¹¶åå®Œæ•´æ¨¡å‹çš„ä¿å­˜è·¯å¾„

    xtuner convert merge \
        /path/to/your/Qwen1.5-1.8B-Chat \
        work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/iter_7000_hf_adapter \
        work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/Qwen1.5-1.8B-Chat-Emotion-Merged
    ```

## ğŸš€ æ¨¡å‹éƒ¨ç½²ä¸æµ‹è¯• (LMDeploy & Streamlit)

1.  **å‡†å¤‡å¯¹è¯æ¨¡æ¿ (`chat_template.json`)ï¼š**
    XTunerè®­ç»ƒæ—¶ä½¿ç”¨çš„å¯¹è¯æ¨¡æ¿ (`xtuner/utils/templates.py`ä¸­çš„`qwen_chat`) å¯èƒ½ä¸LMDeployä¸å®Œå…¨å…¼å®¹ã€‚æ•™ç¨‹ä¸­æä¾›äº†ä¸€ä¸ª`universal_converter.py`è„šæœ¬ï¼ˆæˆ–ç±»ä¼¼é€»è¾‘ï¼‰å°†XTunerçš„æ¨¡æ¿è½¬æ¢ä¸ºLMDeployå…¼å®¹çš„JSONæ ¼å¼ã€‚
    *   **XTuner `qwen_chat` æ¨¡æ¿ (å‚è€ƒ):**
        ```python
        qwen_chat=dict(
            SYSTEM=("<|im_start|>system\n{system}<|im_end|>\n"),
            INSTRUCTION=("<|im_start|>user\n{input}<|im_end|>\n" "<|im_start|>assistant\n"),
            SUFFIX="<|im_end|>",
            SUFFIX_AS_EOS=True,
            SEP="\n",
            STOP_WORDS=["<|im_end|>", "<|endoftext|>"],
        )
        ```
    *   **è½¬æ¢å `outputs/chat_template.json` (ç¤ºä¾‹):**
        ```json
        {
          "meta_instruction": "You are a helpful assistant.",
          "capability": "chat",
          "eosys": "<|im_end|>\n",
          "eoh": "<|im_end|>\n",
          "system": "<|im_start|>system\n{{ system }}<|im_end|>\n",
          "user": "<|im_start|>user\n{{ input }}<|im_end|>",
          "assistant": "<|im_start|>assistant\n",
          "eoa": "<|im_end|>",
          "separator": "\n",
          "stop_words": [
            "<|im_end|>",
            "<|endoftext|>"
          ]
        }
        ```
    *   è¿è¡Œè½¬æ¢è„šæœ¬ (å‡è®¾åä¸º `universal_converter.py` ä¸”å·²æ”¾ç½®åœ¨ `emotion_dialogue_tuner/src/`):
        ```bash
        python emotion_dialogue_tuner/src/universal_converter.py
        ```
        è¿™å°†ç”Ÿæˆ `chat_template.json` æ–‡ä»¶ã€‚

2.  **ä½¿ç”¨LMDeployå¯åŠ¨APIæœåŠ¡ï¼š**
    ```bash
    # MODEL_PATH: åˆå¹¶åçš„å®Œæ•´æ¨¡å‹è·¯å¾„
    # CHAT_TEMPLATE_PATH: ä¸Šä¸€æ­¥ç”Ÿæˆçš„chat_template.jsonè·¯å¾„

    lmdeploy serve api_server \
        work_dirs/qwen1_5_1_8b_chat_qlora_alpaca_e3/Qwen1.5-1.8B-Chat-Emotion-Merged \
        --chat-template outputs/chat_template.json \
        --server-port 23333 # å¯é€‰ï¼ŒæŒ‡å®šç«¯å£
    ```
    *æ³¨æ„ï¼šæ•™ç¨‹ä¸­æåˆ°è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿åœ¨LMDeployä¸­åŠ è½½é‡åˆ°`'NoneType' object is not subscriptable'`é”™è¯¯ï¼Œæœ€ç»ˆé€šè¿‡ä¸åŠ è½½è‡ªå®šä¹‰æ¨¡æ¿ç»•è¿‡ã€‚å¦‚æœé‡åˆ°æ­¤é—®é¢˜ï¼Œå¯å°è¯•ä¸ä½¿ç”¨ `--chat-template` å‚æ•°ï¼ŒLMDeployä¼šå°è¯•è‡ªåŠ¨æ¨æ–­ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´éé¢„æœŸçš„å¯¹è¯æ ¼å¼ã€‚è¯·æŸ¥é˜…LMDeployæœ€æ–°æ–‡æ¡£è§£å†³æ¨¡æ¿å…¼å®¹æ€§é—®é¢˜ã€‚*

3.  **ä½¿ç”¨OpenAI Pythonå®¢æˆ·ç«¯æµ‹è¯•API (å¯é€‰)ï¼š**
    åˆ›å»ºä¸€ä¸ªPythonè„šæœ¬ (å¦‚ `test_api.py`):
    ```python
    from openai import OpenAI

    client = OpenAI(
        base_url="http://localhost:23333/v1/",  # ç¡®ä¿ç«¯å£ä¸LMDeployæœåŠ¡ä¸€è‡´
        api_key="your_api_key"  # LMDeployæœåŠ¡é€šå¸¸ä¸éœ€è¦çœŸå®API Keyï¼Œä»»æ„å­—ç¬¦ä¸²å³å¯
    )

    chat_history = []
    while True:
        user_input = input("ç”¨æˆ·: ")
        if user_input.lower() == "exit":
            break
        chat_history.append({"role": "user", "content": user_input})
        try:
            completion = client.chat.completions.create(
                model="Qwen1.5-1.8B-Chat-Emotion-Merged", # æ­¤å¤„modelåç§°ä¸LMDeployå¯åŠ¨æ—¶ä¸€è‡´
                messages=chat_history,
                temperature=0.7 # å¯è°ƒæ•´
            )
            response = completion.choices[0].message.content
            print(f"AI: {response}")
            chat_history.append({"role": "assistant", "content": response})
        except Exception as e:
            print(f"Error: {e}")
            break
    ```
    è¿è¡Œ: `python test_api.py`

4.  **ä½¿ç”¨Streamlitè¿›è¡Œäº¤äº’æµ‹è¯• (`emotion_dialogue_tuner/chat_app.py`)ï¼š**
    ç¡®ä¿ `chat_app.py` ä¸­çš„ `base_url` å’Œ `model` åç§°æ­£ç¡®ã€‚
    ```python
    # chat_app.py (éƒ¨åˆ†ä»£ç )
    import streamlit as st
    from openai import OpenAI

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OpenAI(
        base_url="http://localhost:23333/v1/", # ç¡®ä¿ä¸LMDeployæœåŠ¡ç«¯å£ä¸€è‡´
        api_key="your_api_key" # ä»»æ„å­—ç¬¦ä¸²
    )

    st.title("æƒ…ç»ªå¯¹è¯æ¨¡å‹æµ‹è¯• ğŸ’¬")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                # æ³¨æ„ï¼šStreamlitåº”ç”¨ä¸­ï¼Œä¸ºä¿æŒä¸Šä¸‹æ–‡ï¼Œåº”ä¼ é€’å†å²æ¶ˆæ¯
                # ä½†æ•™ç¨‹ä¸­çš„chat_app.pyç¤ºä¾‹æ¯æ¬¡åªå‘é€å½“å‰æ¶ˆæ¯ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•è¿›è¡Œå¤šè½®å¯¹è¯
                # ä»¥ä¸‹ä»£ç ä¿®æ”¹ä¸ºä¼ é€’éƒ¨åˆ†å†å²ï¼ˆæˆ–å…¨éƒ¨ï¼Œå–å†³äºæ¨¡å‹å’ŒAPIæ‰¿å—èƒ½åŠ›ï¼‰
                api_messages = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]

                response_stream = client.chat.completions.create(
                    model="Qwen1.5-1.8B-Chat-Emotion-Merged", # ç¡®ä¿ä¸LMDeployå¯åŠ¨çš„æ¨¡å‹åä¸€è‡´
                    messages=api_messages, # å‘é€åŒ…å«å†å²çš„æ¶ˆæ¯
                    stream=True, # ä½¿ç”¨æµå¼è¾“å‡ºä»¥è·å¾—æ‰“å­—æœºæ•ˆæœ
                )
                for chunk in response_stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                full_response = f"æŠ±æ­‰ï¼Œå‡ºé”™äº†: {e}"
            
        st.session_state.messages.append({"role": "assistant", "content": full_response})
    ```
    å¯åŠ¨Streamlitåº”ç”¨ï¼š
    ```bash
    streamlit run emotion_dialogue_tuner/chat_app.py
    ```
    åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ˜¾ç¤ºçš„åœ°å€è¿›è¡Œäº¤äº’æµ‹è¯•ã€‚

## ğŸ’» å…³é”®ä»£ç å’Œé…ç½®ç¤ºä¾‹

*   **`config/settings.yaml` (APIå¯†é’¥ä¸è·¯å¾„):**
    ```yaml
    API:
      ZHIPU_API_KEY: "your_zhipu_api_key_here"
      MODEL_NAME: "glm-3-turbo"
    PATHS:
      EMBEDDING_MODEL: "path/to/embedding_model/thomas/text2vec-base-chinese"
    ```

*   **`config/style_config.json` (å¯¹è¯é£æ ¼å®šä¹‰):**
    ```json
    {
        "æ¸©æŸ”": {
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´çš„èŠå¤©åŠ©æ‰‹...",
            "examples": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}],
            "temperature": 0.3
        }
        // ...å…¶ä»–é£æ ¼
    }
    ```

*   **XTunerè®­ç»ƒå¯åŠ¨å‘½ä»¤ (ç¤ºä¾‹):**
    ```bash
    xtuner train xtuner_configs/qwen1_5_1_8b_chat_qlora_alpaca_e3.py
    ```

*   **LMDeployæœåŠ¡å¯åŠ¨å‘½ä»¤ (ç¤ºä¾‹):**
    ```bash
    lmdeploy serve api_server work_dirs/.../Qwen1.5-1.8B-Chat-Emotion-Merged --chat-template outputs/chat_template.json
    ```

## ğŸ¤” é‡åˆ°çš„é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆå›é¡¾

1.  **`settings.yaml`é…ç½®æ–‡ä»¶æ‰¾ä¸åˆ°ï¼š**
    *   **åŸå› ï¼š** `config_loader.py`ä¸­`root_path`å±‚çº§é…ç½®é”™è¯¯ã€‚
    *   **è§£å†³ï¼š** ä¿®æ­£`Path(__file__).resolve().parent.parent.parent`çš„å±‚çº§ã€‚
2.  **æ–‡ä»¶ç¼–ç é”™è¯¯ (`UnicodeDecodeError: 'gbk'`)ï¼š**
    *   **åŸå› ï¼š** æ–‡ä»¶è¯»å–æ—¶æœªæŒ‡å®šUTF-8ç¼–ç ã€‚
    *   **è§£å†³ï¼š** åœ¨æ‰€æœ‰`open()`å‡½æ•°ä¸­æ·»åŠ `encoding="utf-8"`ã€‚
3.  **XTunerå¾®è°ƒæŠ¥é”™ `No module named 'triton.ops'`ï¼š**
    *   **åŸå› ï¼š** `bitsandbytes`åº“ä¾èµ–é—®é¢˜ï¼Œé€šå¸¸ä¸PyTorchã€CUDAç‰ˆæœ¬ä¸å…¼å®¹ã€‚
    *   **è§£å†³ï¼š** é™çº§PyTorchç‰ˆæœ¬ï¼ˆå¦‚ `torch==2.2.0+cu121`, `torchvision==0.17.0+cu121`ï¼‰ï¼Œç¡®ä¿CUDAç‰ˆæœ¬åŒ¹é…ï¼ˆæ•™ç¨‹ä¸­ä¸ºCUDA 12.1ï¼‰ï¼Œå¹¶é‡æ–°å®‰è£…XTunerä¾èµ–ã€‚
4.  **LMDeployå¯¼å…¥è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿åæŠ¥é”™ (`'NoneType' object is not subscriptable`)ï¼š**
    *   **åŸå› ï¼š** è‡ªå®šä¹‰çš„`chat_template.json`æ ¼å¼æˆ–å†…å®¹é—®é¢˜ã€‚
    *   **è§£å†³/è§„é¿ï¼š** æ•™ç¨‹ä¸­æœªå®Œå…¨è§£å†³ï¼Œæµ‹è¯•æ—¶é€šè¿‡ä¸åŠ è½½è‡ªå®šä¹‰æ¨¡æ¿ç»•è¿‡ã€‚éœ€ä»”ç»†æ£€æŸ¥JSONæ ¼å¼æˆ–å‚è€ƒLMDeployæœ€æ–°æ–‡æ¡£ã€‚
5.  **æ¨¡å‹è¿‡æ‹Ÿåˆï¼š**
    *   **åŸå› ï¼š** è®­ç»ƒè½®æ•°è¿‡å¤šï¼Œlossè¿‡ä½ã€‚
    *   **è§£å†³ï¼š** æå‰åœæ­¢è®­ç»ƒï¼ˆå¦‚6000-7000è½®æ¬¡ï¼Œlossåœ¨0.03-0.04ï¼‰ï¼Œç›‘æ§éªŒè¯é›†è¡¨ç°ã€‚è°ƒæ•´XTuneré…ç½®ä»¥ä¿å­˜æ›´å¤šä¸­é—´æƒé‡ã€‚

## ğŸ’¡ é¡¹ç›®æ€»ç»“

æœ¬é¡¹ç›®æˆåŠŸå®è·µäº†ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„LLMå¾®è°ƒå…¨æµç¨‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå…·å¤‡ç‰¹å®šæƒ…ç»ªè¡¨è¾¾èƒ½åŠ›çš„å¯¹è¯æ¨¡å‹ã€‚ä¸»è¦æ”¶è·åŒ…æ‹¬ï¼š

*   **æ•°æ®è´¨é‡æ˜¯å…³é”®ï¼š** é«˜è´¨é‡ã€ç›®æ ‡æ˜ç¡®çš„æ•°æ®é›†å¯¹å¾®è°ƒæ•ˆæœè‡³å…³é‡è¦ã€‚
*   **ç¯å¢ƒé…ç½®æŒ‘æˆ˜ï¼š** ä¾èµ–åŒ…ç‰ˆæœ¬ï¼ˆå°¤å…¶æ˜¯PyTorch, CUDA, bitsandbytesï¼‰çš„å…¼å®¹æ€§æ˜¯å¸¸è§éš¾ç‚¹ã€‚
*   **è¿­ä»£ä¸è°ƒä¼˜ï¼š** æ¨¡å‹è®­ç»ƒæ˜¯ä¸€ä¸ªä¸æ–­è¿­ä»£å’Œè°ƒæ•´å‚æ•°çš„è¿‡ç¨‹ï¼Œéœ€è¦è€å¿ƒå’Œç»†è‡´çš„è§‚å¯Ÿã€‚
*   **å·¥å…·é“¾ç†Ÿæ‚‰åº¦ï¼š** æŒæ¡XTuner, LMDeployç­‰å·¥å…·çš„ä½¿ç”¨èƒ½æå¤§æé«˜æ•ˆç‡ã€‚

## ğŸš€ æœªæ¥å±•æœ›

1.  **è¯­éŸ³å¯¹è¯é›†æˆï¼š** ç»“åˆTTSå’ŒASRæŠ€æœ¯ï¼Œå®ç°è¯­éŸ³äº¤äº’çš„æƒ…ç»ªå¯¹è¯æœºå™¨äººã€‚
2.  **éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡ï¼š** æ¢ç´¢å°†æ¨¡å‹é‡åŒ–ã€å‰ªæåéƒ¨ç½²åˆ°å¼€å‘æ¿æˆ–åµŒå…¥å¼è®¾å¤‡ã€‚
3.  **æ›´ç»†è‡´çš„æƒ…ç»ªæ§åˆ¶ï¼š** ç ”ç©¶æ›´å¤æ‚çš„æƒ…ç»ªåˆ†ç±»å’Œç”Ÿæˆå¼ºåº¦æ§åˆ¶ã€‚
4.  **è¯„ä¼°ä½“ç³»å®Œå–„ï¼š** å¼•å…¥æ›´å…¨é¢çš„äººå·¥è¯„ä¼°å’Œé’ˆå¯¹æƒ…ç»ªè¡¨è¾¾çš„å®¢è§‚è¯„ä¼°æŒ‡æ ‡ã€‚

## ğŸ“š å‚è€ƒèµ„æ–™ä¸é“¾æ¥

*   **æ™ºè°±AIå¼€æ”¾å¹³å°:** [https://www.bigmodel.cn/console/overview](https://www.bigmodel.cn/console/overview)
*   **XTunerå®˜æ–¹æ–‡æ¡£:** [https://xtuner.readthedocs.io/zh-cn/latest/](https://xtuner.readthedocs.io/zh-cn/latest/)
*   **OpenCompasså®˜æ–¹æ–‡æ¡£:** [https://doc.opencompass.org.cn/zh_CN/](https://doc.opencompass.org.cn/zh_CN/)
*   **LMDeployå®˜æ–¹æ–‡æ¡£:** [https://lmdeploy.readthedocs.io/zh-cn/latest/](https://lmdeploy.readthedocs.io/zh-cn/latest/)
*   **QwenLM (Qwen1.5):** [https://github.com/QwenLM/Qwen1.5](https://github.com/QwenLM/Qwen1.5)
*   **Streamlitå®˜æ–¹æ–‡æ¡£:** [https://docs.streamlit.io/](https://docs.streamlit.io/)
*   **æ•°æ®é›†å‚è€ƒ:**
    *   CDial-GPT: [https://github.com/thu-coai/CDial-GPT](https://github.com/thu-coai/CDial-GPT) (åŸæ•™ç¨‹æåŠï¼Œä½†å®é™…æ˜¯corpus-dataset/CDial-GPT)
    *   LCCC (é­”å¡”ç¤¾åŒº): [https://www.modelscope.cn/datasets/OmniData/LCCC](https://www.modelscope.cn/datasets/OmniData/LCCC)
    *   text2vec-base-chinese (åµŒå…¥æ¨¡å‹): [https://modelscope.cn/models/thomas/text2vec-base-chinese/summary](https://modelscope.cn/models/thomas/text2vec-base-chinese/summary)

---

**å…è´£å£°æ˜:** æœ¬READMEæ ¹æ®ä¸ªäººç¼–å†™çš„é¡¹ç›®æ“ä½œæ•™ç¨‹ç„¶åé€šè¿‡AIåˆ†ææ•´ç†è€Œæˆã€‚æ‰€æœ‰è·¯å¾„ã€APIå¯†é’¥å’Œå…·ä½“å‚æ•°è¯·æ ¹æ®æ‚¨è‡ªå·±çš„å®é™…ç¯å¢ƒå’Œéœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚åœ¨æ‰§è¡Œä»»ä½•å‘½ä»¤å‰ï¼Œè¯·ç¡®ä¿å·²å¤‡ä»½é‡è¦æ•°æ®ã€‚

å¦‚æœ‰é—®é¢˜æˆ–è€…ç›¸åº”æ›´è¯¦ç»†çš„æ•™ç¨‹å¯ä»¥æŸ¥çœ‹æœ¬äººåŸæ•™ç¨‹åœ°å€ï¼š[ã€é¡¹ç›®å®æˆ˜ã€‘å¤§æ¨¡å‹å¾®è°ƒ-æƒ…ç»ªå¯¹è¯æ¨¡å‹](https://blog.ismoyu.cn/xiang-mu-shi-zhan-da-mo-xing-wei-diao-qing-xu-dui-hua-mo-xing/)
